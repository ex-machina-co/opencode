---
title: 공급자
description: OpenCode에서 LLM 공급자를 사용합니다.
---

import config from "../../../../config.mjs"
export const console = config.console

OpenCode는 [AI SDK](https://ai-sdk.dev/) 및 [Models.dev](https://models.dev)를 사용하여 **75개 이상의 LLM 공급자**를 지원하며 로컬 모델도 실행할 수 있습니다.

필요한 공급자를 추가하려면:

1. `/connect` 명령을 사용하여 공급자의 API 키를 추가하십시오.
2. OpenCode 구성에서 공급자를 구성하십시오.

---

### 자격 증명

`/connect` 명령으로 공급자의 API 키를 추가하면 다음 위치에 저장됩니다:
`~/.local/share/opencode/auth.json`

---

### 구성

OpenCode 설정의 `provider` 섹션을 통해 공급자를 사용자 정의할 수 있습니다.

---

#### 기본 URL

`baseURL` 옵션을 설정하여 모든 공급자를 위한 기본 URL을 사용자 정의할 수 있습니다. 프록시 서비스 또는 사용자 정의 엔드포인트를 사용할 때 유용합니다.

```json title="opencode.json" {6}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "anthropic": {
      "options": {
        "baseURL": "https://api.anthropic.com/v1"
      }
    }
  }
}
```

---

## OpenCode Zen

OpenCode Zen은 OpenCode 팀이 OpenCode와 잘 작동하도록 테스트하고 검증한 모델 목록입니다. [더 알아보기](/docs/zen).

:::tip
처음이라면 OpenCode Zen으로 시작하는 것이 좋습니다.
:::

1. TUI에서 `/connect` 명령을 실행하고 `OpenCode Zen`을 선택한 뒤, [opencode.ai/auth](https://opencode.ai/zen)로 이동합니다.

   ```txt
   /connect
   ```

2. 로그인하고 결제 정보를 입력한 후 API 키를 복사하십시오.

3. API 키를 붙여넣습니다.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. TUI에서 `/models`를 실행하여 추천 모델 목록을 볼 수 있습니다.

   ```txt
   /models
   ```

OpenCode의 다른 공급자처럼 작동하며 사용은 완전히 선택 사항입니다.

---

## OpenCode Go

OpenCode Go는 OpenCode 팀이 테스트하고 검증하여 OpenCode와 잘 작동하는 인기 있는 오픈 코딩 모델에 안정적으로 액세스할 수 있는 저렴한 구독 요금제입니다.

1. TUI에서 `/connect` 명령을 실행하고 `OpenCode Go`를 선택한 뒤 [opencode.ai/auth](https://opencode.ai/zen)로 이동하십시오.

   ```txt
   /connect
   ```

2. 로그인하고 결제 정보를 입력한 후 API 키를 복사하십시오.

3. API 키를 붙여넣습니다.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. TUI에서 `/models`를 실행하여 추천 모델 목록을 볼 수 있습니다.

   ```txt
   /models
   ```

OpenCode의 다른 공급자처럼 작동하며 사용은 완전히 선택 사항입니다.

---

## 디렉토리

공급자 세부 정보를 확인하세요. 목록에 공급자를 추가하려면 PR을 열어주세요.

:::note
원하는 공급자가 없나요? PR을 제출해 주세요.
:::

---

### 302.AI

1. [302.AI 콘솔](https://302.ai/)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **302.AI**를 검색하십시오.

   ```txt
   /connect
   ```

3. 302.AI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

---

### Amazon Bedrock

OpenCode로 Amazon Bedrock을 사용하려면:

1. Amazon Bedrock 콘솔의 **Model catalog**로 이동하여 원하는 모델에 대한 액세스를 요청합니다.

   :::tip
   Amazon Bedrock에서 원하는 모델에 대한 액세스 권한이 있어야 합니다.
   :::

2. 다음 방법 중 하나를 사용하여 **인증을 구성**합니다:

   ***

   #### 환경 변수 (빠른 시작)

   OpenCode를 실행하는 동안 다음 환경 변수 중 하나를 설정합니다:

   ```bash
   # Option 1: Using AWS access keys
   AWS_ACCESS_KEY_ID=XXX AWS_SECRET_ACCESS_KEY=YYY opencode

   # Option 2: Using named AWS profile
   AWS_PROFILE=my-profile opencode

   # Option 3: Using Bedrock bearer token
   AWS_BEARER_TOKEN_BEDROCK=XXX opencode
   ```

   또는 bash 프로필에 추가합니다:

   ```bash title="~/.bash_profile"
   export AWS_PROFILE=my-dev-profile
   export AWS_REGION=us-east-1
   ```

   ***

   #### 구성 파일 (권장)

   프로젝트별 또는 영구 구성을 위해 `opencode.json`을 사용하십시오:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "my-aws-profile"
         }
       }
     }
   }
   ```

   **유효한 옵션:**
   - `region` - AWS 리전 (예: `us-east-1`, `eu-west-1`)
   - `profile` - `~/.aws/credentials`의 AWS 프로필 이름
   - `endpoint` - VPC 엔드포인트 등을 위한 사용자 정의 엔드포인트 URL (일반 `baseURL` 옵션의 별칭)

   :::tip
   구성 파일 옵션은 환경 변수보다 우선 순위가 높습니다.
   :::

   ***

   #### 고급: VPC 엔드포인트

   Bedrock의 VPC 엔드포인트를 사용하는 경우:

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "amazon-bedrock": {
         "options": {
           "region": "us-east-1",
           "profile": "production",
           "endpoint": "https://bedrock-runtime.us-east-1.vpce-xxxxx.amazonaws.com"
         }
       }
     }
   }
   ```

   :::note
   `endpoint` 옵션은 일반적인 `baseURL` 옵션의 별칭입니다. `endpoint`와 `baseURL` 둘 다 지정된 경우 `endpoint`가 우선합니다.
   :::

   ***

   #### 인증 방법
   - **`AWS_ACCESS_KEY_ID` / `AWS_SECRET_ACCESS_KEY`**: IAM 사용자 및 AWS 콘솔에서 액세스 키 생성
   - **`AWS_PROFILE`**: `~/.aws/credentials`의 프로필 이름을 사용합니다. `aws configure --profile my-profile` 또는 `aws sso login`으로 먼저 구성하십시오.
   - **`AWS_BEARER_TOKEN_BEDROCK`**: Amazon Bedrock 콘솔에서 임시 API 키 생성
   - **`AWS_WEB_IDENTITY_TOKEN_FILE` / `AWS_ROLE_ARN`**: EKS IRSA (서비스 계정용 IAM 역할) 또는 다른 Kubernetes 환경의 OIDC 연동. 이 환경 변수는 서비스 계정을 사용할 때 Kubernetes에 의해 자동으로 주입됩니다.

   ***

   #### 인증 우선 순위

   Amazon Bedrock은 다음과 같은 인증 우선 순위를 사용합니다.
   1. **Bearer Token** - `AWS_BEARER_TOKEN_BEDROCK` 환경 변수 또는 `/connect` 명령의 토큰
   2. **AWS Credential Chain** - 프로필, 액세스 키, 공유 자격 증명, IAM 역할, 웹 ID 토큰 (EKS IRSA), 인스턴스 메타데이터

   :::note
   Bearer 토큰을 설정할 때 (`/connect` 또는 `AWS_BEARER_TOKEN_BEDROCK`를 통해), 구성된 프로필을 포함한 모든 AWS 자격 증명 방법보다 우선 순위가 높습니다.
   :::

3. `/models` 명령을 실행하여 원하는 모델을 선택하십시오.

   ```txt
   /models
   ```

:::note
사용자 정의 추론 프로필(custom inference profiles)의 경우, 키에 모델과 공급자 이름을 사용하고 `id` 속성에 ARN을 설정하십시오. 이렇게 하면 올바른 캐싱이 보장됩니다.
:::

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "amazon-bedrock": {
      // ...
      "models": {
        "anthropic-claude-sonnet-4.5": {
          "id": "arn:aws:bedrock:us-east-1:xxx:application-inference-profile/yyy"
        }
      }
    }
  }
}
```

---

### Anthropic

1. 가입 후 `/connect` 명령을 실행하고 **Anthropic**을 선택합니다.

   ```txt
   /connect
   ```

2. **Claude Pro/Max** 옵션을 선택하면 브라우저가 열리고 인증을 요청합니다.

   ```txt
   ┌ Select auth method
   │
   │ Claude Pro/Max
   │ Create an API Key
   │ Manually enter API Key
   └
   ```

3. 이제 `/models` 명령을 사용할 때 모든 Anthropic 모델을 사용할 수 있습니다.

   ```txt
   /models
   ```

:::info
OpenCode에서 Claude Pro/Max 구독을 사용하는 것은 [Anthropic](https://anthropic.com)에서 공식적으로 지원하지 않습니다.
:::

##### API 키 사용

Pro/Max 구독이 없는 경우 **Create an API Key**를 선택할 수 있습니다. 브라우저가 열리고 Anthropic에 로그인한 후 터미널에 붙여넣을 수 있는 코드를 제공합니다.

또는 이미 API 키가 있다면, **Manually enter API Key**를 선택하고 터미널에 붙여넣을 수 있습니다.

---

### Azure OpenAI

:::note
"I'm sorry, but I cannot assist with that request" 오류가 발생하면, Azure 리소스의 콘텐츠 필터를 **DefaultV2**에서 **Default**로 변경해 보세요.
:::

1. [Azure 포털](https://portal.azure.com/)로 이동하여 **Azure OpenAI** 리소스를 만듭니다. 다음이 필요합니다:
   - **리소스 이름**: API 엔드포인트의 일부가 됩니다 (`https://RESOURCE_NAME.openai.azure.com/`)
   - **API 키**: 리소스의 `KEY 1` 또는 `KEY 2`

2. [Azure AI Foundry](https://ai.azure.com/)로 이동하여 모델을 배포합니다.

   :::note
   배포 이름은 제대로 작동하려면 OpenCode의 모델 이름과 일치해야 합니다.
   :::

3. `/connect` 명령을 실행하고 **Azure**를 검색하십시오.

   ```txt
   /connect
   ```

4. API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 리소스 이름을 환경 변수로 설정:

   ```bash
   AZURE_RESOURCE_NAME=XXX opencode
   ```

   또는 bash 프로필에 추가:

   ```bash title="~/.bash_profile"
   export AZURE_RESOURCE_NAME=XXX
   ```

6. `/models` 명령을 실행하여 배포된 모델을 선택하십시오.

   ```txt
   /models
   ```

---

### Azure Cognitive Services

1. [Azure 포털](https://portal.azure.com/)로 이동하여 **Azure OpenAI** 리소스를 만듭니다. 다음이 필요합니다:
   - **리소스 이름**: API 엔드포인트의 일부가 됩니다 (`https://AZURE_COGNITIVE_SERVICES_RESOURCE_NAME.cognitiveservices.azure.com/`)
   - **API 키**: 리소스의 `KEY 1` 또는 `KEY 2`

2. [Azure AI Foundry](https://ai.azure.com/)로 이동하여 모델을 배포합니다.

   :::note
   배포 이름은 제대로 작동하려면 OpenCode의 모델 이름과 일치해야 합니다.
   :::

3. `/connect` 명령을 실행하고 **Azure Cognitive Services**를 검색하십시오.

   ```txt
   /connect
   ```

4. API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

5. 리소스 이름을 환경 변수로 설정:

   ```bash
   AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX opencode
   ```

   또는 bash 프로필에 추가:

   ```bash title="~/.bash_profile"
   export AZURE_COGNITIVE_SERVICES_RESOURCE_NAME=XXX
   ```

6. `/models` 명령을 실행하여 배포된 모델을 선택하십시오.

   ```txt
   /models
   ```

---

### Baseten

1. [Baseten](https://app.baseten.co/)으로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Baseten**을 검색하십시오.

   ```txt
   /connect
   ```

3. Baseten API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

---

### Cerebras

1. [Cerebras 콘솔](https://inference.cerebras.ai/)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Cerebras**를 검색하십시오.

   ```txt
   /connect
   ```

3. Cerebras API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Qwen 3 Coder 480B_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Cloudflare AI Gateway

Cloudflare AI Gateway는 OpenAI, Anthropic, Workers AI 등의 모델에 액세스할 수 있으며, 통합된 엔드포인트를 통해 더 많은 기능을 제공합니다. [Unified Billing](https://developers.cloudflare.com/ai-gateway/features/unified-billing/)을 사용하면 각 공급자의 별도 API 키가 필요하지 않습니다.

1. [Cloudflare 대시보드](https://dash.cloudflare.com/)로 이동하여, **AI** > **AI Gateway**로 가서 새로운 게이트웨이를 만듭니다.

2. 계정 ID 및 Gateway ID를 환경 변수로 설정하십시오.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_ACCOUNT_ID=your-32-character-account-id
   export CLOUDFLARE_GATEWAY_ID=your-gateway-id
   ```

3. `/connect` 명령을 실행하고 **Cloudflare AI Gateway**를 검색하십시오.

   ```txt
   /connect
   ```

4. Cloudflare API 토큰을 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

   또는 환경 변수로 설정합니다.

   ```bash title="~/.bash_profile"
   export CLOUDFLARE_API_TOKEN=your-api-token
   ```

5. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

   OpenCode 구성을 통해 모델을 추가할 수 있습니다.

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "cloudflare-ai-gateway": {
         "models": {
           "openai/gpt-4o": {},
           "anthropic/claude-sonnet-4": {}
         }
       }
     }
   }
   ```

---

### Cortecs

1. [Cortecs 콘솔](https://cortecs.ai/)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Cortecs**를 검색하십시오.

   ```txt
   /connect
   ```

3. Cortecs API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Kimi K2 Instruct_)을 선택하십시오.

   ```txt
   /models
   ```

---

### DeepSeek

1. [DeepSeek 콘솔](https://platform.deepseek.com/)로 이동하여 계정을 만들고 **API Keys**를 클릭하여 키를 생성합니다.

2. `/connect` 명령을 실행하고 **DeepSeek**를 검색하십시오.

   ```txt
   /connect
   ```

3. DeepSeek API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 DeepSeek 모델(예: _DeepSeek Reasoner_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Deep Infra

1. [Deep Infra 대시보드](https://deepinfra.com/dash)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Deep Infra**를 검색하십시오.

   ```txt
   /connect
   ```

3. Deep Infra API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

---

### Firmware

1. [Firmware 대시보드](https://app.firmware.ai/signup)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Firmware**를 검색하십시오.

   ```txt
   /connect
   ```

3. Firmware API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

---

### Fireworks AI

1. [Fireworks AI 콘솔](https://app.fireworks.ai/)로 이동하여 계정을 만들고 **API Keys**를 클릭합니다.

2. `/connect` 명령을 실행하고 **Fireworks AI**를 검색하십시오.

   ```txt
   /connect
   ```

3. Fireworks AI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Kimi K2 Instruct_)을 선택하십시오.

   ```txt
   /models
   ```

---

### GitLab Duo

GitLab Duo는 GitLab의 Anthropic 프록시를 통해 기본 도구 호출 기능을 갖춘 AI 기반 에이전트 채팅을 제공합니다.

1. `/connect` 명령을 실행하고 GitLab을 선택합니다.

   ```txt
   /connect
   ```

2. 인증 방법을 선택하십시오:

   ```txt
   ┌ Select auth method
   │
   │ OAuth (Recommended)
   │ Personal Access Token
   └
   ```

   #### OAuth 사용 (권장)

   **OAuth**를 선택하면 브라우저가 열리고 인증을 요청합니다.

   #### 개인 액세스 토큰 사용
   1. [GitLab User Settings > Access Tokens](https://gitlab.com/-/user_settings/personal_access_tokens)로 이동
   2. **Add new token** 클릭
   3. 이름: `OpenCode`, 범위: `api`
   4. 토큰 복사 (`glpat-`로 시작)
   5. 터미널에 입력

3. `/models` 명령을 실행하여 사용 가능한 모델을 확인하십시오.

   ```txt
   /models
   ```

   세 가지 Claude 기반 모델을 사용할 수 있습니다:
   - **duo-chat-haiku-4-5** (기본값) - 빠른 작업을 위한 빠른 응답
   - **duo-chat-sonnet-4-5** - 대부분의 워크플로우에 균형 잡힌 성능
   - **duo-chat-opus-4-5** - 복잡한 분석에 적합

:::note
`GITLAB_TOKEN` 환경 변수를 지정하여 토큰을 저장하지 않고 사용할 수도 있습니다.
:::

#### 셀프 호스팅 GitLab

:::note[규정 준수 참고 사항]
OpenCode는 세션 제목 생성과 같은 일부 AI 작업을 위해 작은 모델을 사용합니다.
기본적으로 Zen에서 호스팅되는 gpt-5-nano를 사용하도록 구성됩니다.
OpenCode를 자체 호스팅 GitLab 인스턴스만 사용하도록 제한하려면 `opencode.json` 파일에 다음을 추가하십시오.
세션 공유를 비활성화하는 것도 권장합니다.

```json
{
  "$schema": "https://opencode.ai/config.json",
  "small_model": "gitlab/duo-chat-haiku-4-5",
  "share": "disabled"
}
```

:::

자체 호스팅 GitLab 인스턴스의 경우:

```bash
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_TOKEN=glpat-...
```

인스턴스가 사용자 정의 AI Gateway를 실행하는 경우:

```bash
GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
```

또는 bash 프로필에 추가:

```bash title="~/.bash_profile"
export GITLAB_INSTANCE_URL=https://gitlab.company.com
export GITLAB_AI_GATEWAY_URL=https://ai-gateway.company.com
export GITLAB_TOKEN=glpat-...
```

:::note
GitLab 관리자는 다음을 활성화해야 합니다:

1. [Duo Agent Platform](https://docs.gitlab.com/user/gitlab_duo/turn_on_off/) (사용자, 그룹 또는 인스턴스)
2. 기능 플래그 (Rails 콘솔을 통해):
   - `agent_platform_claude_code`
   - `third_party_agents_enabled`
     :::

#### 셀프 호스팅 인스턴스용 OAuth

자체 호스팅 인스턴스에 대해 OAuth를 작동시키려면 새로운 애플리케이션(Settings → Applications)을 만들어야 합니다.
콜백 URL `http://127.0.0.1:8080/callback` 및 다음 범위가 필요합니다:

- api (사용자 대신 API 액세스)
- read_user (개인 정보 읽기)
- read_repository (리포지토리 읽기 전용 액세스)

그런 다음 애플리케이션 ID를 환경 변수로 노출하십시오:

```bash
export GITLAB_OAUTH_CLIENT_ID=your_application_id_here
```

[opencode-gitlab-auth](https://www.npmjs.com/package/@gitlab/opencode-gitlab-auth) 홈페이지에 추가 문서가 있습니다.

#### 구성

`opencode.json`을 통해 사용자 정의:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "gitlab": {
      "options": {
        "instanceUrl": "https://gitlab.com",
        "featureFlags": {
          "duo_agent_platform_agentic_chat": true,
          "duo_agent_platform": true
        }
      }
    }
  }
}
```

#### GitLab API 도구 (선택 사항이지만 강력 권장)

GitLab 도구(병합 요청, 이슈, 파이프라인, CI/CD 등)에 액세스하려면:

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "plugin": ["@gitlab/opencode-gitlab-plugin"]
}
```

이 플러그인은 MR 리뷰, 이슈 추적, 파이프라인 모니터링 등을 포함한 포괄적인 GitLab 리포지토리 관리 기능을 제공합니다.

---

### GitHub Copilot

OpenCode에서 GitHub Copilot 구독을 사용하려면:

:::note
일부 모델은 [Pro+ 구독](https://github.com/features/copilot/plans)이 필요할 수 있습니다.
:::

1. `/connect` 명령을 실행하고 GitHub Copilot을 검색하십시오.

   ```txt
   /connect
   ```

2. [github.com/login/device](https://github.com/login/device)로 이동하여 코드를 입력합니다.

   ```txt
   ┌ Login with GitHub Copilot
   │
   │ https://github.com/login/device
   │
   │ Enter code: 8F43-6FCF
   │
   │ Waiting for authorization...
   └
   ```

3. 이제 원하는 모델을 선택하기 위해 `/models` 명령을 실행합니다.

   ```txt
   /models
   ```

---

### Google Vertex AI

OpenCode로 Google Vertex AI를 사용하려면:

1. Google Cloud Console의 **Model Garden**으로 이동하여 해당 리전에서 사용 가능한 모델을 확인하십시오.

   :::note
   Vertex AI API가 활성화된 Google Cloud 프로젝트가 있어야 합니다.
   :::

2. 필요한 환경 변수를 설정:
   - `GOOGLE_CLOUD_PROJECT`: 구글 클라우드 프로젝트 ID
   - `VERTEX_LOCATION` (선택): Vertex AI 리전 (기본값: `global`)
   - 인증 (하나 선택):
     - `GOOGLE_APPLICATION_CREDENTIALS`: 서비스 계정 JSON 키 파일 경로
     - gcloud CLI 사용: `gcloud auth application-default login`

   OpenCode를 실행할 때 설정:

   ```bash
   GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json GOOGLE_CLOUD_PROJECT=your-project-id opencode
   ```

   또는 bash 프로필에 추가:

   ```bash title="~/.bash_profile"
   export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
   export GOOGLE_CLOUD_PROJECT=your-project-id
   export VERTEX_LOCATION=global
   ```

:::tip
`global` 리전은 가용성을 높이고 오류를 줄이며 추가 비용이 없습니다. 데이터 거주 요건이 있는 경우 지역 엔드포인트(예: `us-central1`)를 사용하십시오. [더 알아보기](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-partner-models#regional_and_global_endpoints)
:::

3. `/models` 명령을 실행하여 원하는 모델을 선택하십시오.

   ```txt
   /models
   ```

---

### Groq

1. [Groq 콘솔](https://console.groq.com/)로 이동하여 **Create API Key**를 클릭하고 키를 복사합니다.

2. `/connect` 명령을 실행하고 Groq를 검색하십시오.

   ```txt
   /connect
   ```

3. API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 원하는 모델을 선택합니다.

   ```txt
   /models
   ```

---

### Hugging Face

[Hugging Face Inference Providers](https://huggingface.co/docs/inference-providers)는 17개 이상의 공급자가 지원하는 오픈 모델에 대한 액세스를 제공합니다.

1. [Hugging Face settings](https://huggingface.co/settings/tokens/new?ownUserPermissions=inference.serverless.write&tokenType=fineGrained)로 이동하여 Inference Providers에 호출할 권한이 있는 토큰을 생성합니다.

2. `/connect` 명령을 실행하고 **Hugging Face**를 검색하십시오.

   ```txt
   /connect
   ```

3. Hugging Face 토큰을 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Kimi-K2-Instruct_ 또는 _GLM-4.6_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Helicone

[Helicone](https://helicone.ai)는 AI 애플리케이션을 위한 로깅, 모니터링 및 분석 기능을 제공하는 LLM 관찰 가능성(Observability) 플랫폼입니다. Helicone AI Gateway는 모델을 기반으로 적절한 공급자에게 요청을 자동으로 라우팅합니다.

1. [Helicone](https://helicone.ai)로 이동하여 계정을 만들고 대시보드에서 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Helicone**를 검색하십시오.

   ```txt
   /connect
   ```

3. Helicone API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델을 선택하십시오.

   ```txt
   /models
   ```

캐싱 및 속도 제한과 같은 더 많은 공급자와 고급 기능은 [Helicone 문서](https://docs.helicone.ai)를 확인하십시오.

#### 선택적 구성

OpenCode를 통해 자동으로 구성되지 않는 Helicone의 기능이나 모델이 있는 경우 직접 구성할 수 있습니다.

[Helicone의 모델 디렉토리](https://helicone.ai/models)에서 추가하려는 모델의 ID를 확인하십시오.

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
      },
      "models": {
        "gpt-4o": {
          // Model ID (from Helicone's model directory page)
          "name": "GPT-4o", // Your own custom name for the model
        },
        "claude-sonnet-4-20250514": {
          "name": "Claude Sonnet 4",
        },
      },
    },
  },
}
```

#### 사용자 정의 헤더

Helicone는 캐싱, 사용자 추적 및 세션 관리와 같은 기능을 위한 사용자 정의 헤더를 지원합니다. `options.headers`를 사용하여 공급자 구성에 추가하십시오:

```jsonc title="~/.config/opencode/opencode.jsonc"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "helicone": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Helicone",
      "options": {
        "baseURL": "https://ai-gateway.helicone.ai",
        "headers": {
          "Helicone-Cache-Enabled": "true",
          "Helicone-User-Id": "opencode",
        },
      },
    },
  },
}
```

##### 세션 추적

Helicone의 [Sessions](https://docs.helicone.ai/features/sessions) 기능을 사용하면 관련 LLM 요청을 그룹화할 수 있습니다. [opencode-helicone-session](https://github.com/H2Shami/opencode-helicone-session) 플러그인을 사용하여 각 OpenCode 대화를 Helicone 세션으로 자동 기록하십시오.

```bash
npm install -g opencode-helicone-session
```

설정에 추가하십시오.

```json title="opencode.json"
{
  "plugin": ["opencode-helicone-session"]
}
```

이 플러그인은 `Helicone-Session-Id` 및 `Helicone-Session-Name` 헤더를 요청에 주입합니다. Helicone의 세션 페이지에서 각 OpenCode 대화가 별도의 세션으로 나열되는 것을 볼 수 있습니다.

##### 공통 Helicone 헤더

| 헤더                       | 설명                                                       |
| -------------------------- | ---------------------------------------------------------- |
| `Helicone-Cache-Enabled`   | 응답 캐싱 활성화 (`true`/`false`)                          |
| `Helicone-User-Id`         | 사용자별 지표 추적                                         |
| `Helicone-Property-[Name]` | 사용자 정의 속성 추가(예: `Helicone-Property-Environment`) |
| `Helicone-Prompt-Id`       | 요청을 프롬프트 버전과 연관                                |

사용 가능한 모든 헤더는 [Helicone Header Directory](https://docs.helicone.ai/helicone-headers/header-directory)를 참조하십시오.

---

### llama.cpp

[llama.cpp](https://github.com/ggml-org/llama.cpp)의 llama-server 유틸리티를 통해 로컬 모델을 사용하도록 구성할 수 있습니다.

```json title="opencode.json" "llama.cpp" {5, 6, 8, 10-15}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "llama.cpp": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "llama-server (local)",
      "options": {
        "baseURL": "http://127.0.0.1:8080/v1"
      },
      "models": {
        "qwen3-coder:a3b": {
          "name": "Qwen3-Coder: a3b-30b (local)",
          "limit": {
            "context": 128000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

이 예제에서:

- `llama.cpp`는 사용자 정의 공급자 ID입니다. 원하는 문자열로 지정할 수 있습니다.
- `npm`은 이 공급자에 사용할 패키지를 지정합니다. 여기서는 OpenAI 호환 API를 위해 `@ai-sdk/openai-compatible`을 사용합니다.
- `name`은 UI에 표시될 공급자 이름입니다.
- `options.baseURL`은 로컬 서버의 엔드포인트입니다.
- `models`는 모델 ID와 해당 구성을 매핑합니다. 모델 이름은 모델 선택 목록에 표시됩니다.

---

### IO.NET

IO.NET은 다양한 사용 사례에 최적화된 17개의 모델을 제공합니다:

1. [IO.NET 콘솔](https://ai.io.net/)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **IO.NET**을 검색하십시오.

   ```txt
   /connect
   ```

3. IO.NET API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 모델을 선택하려면 `/models` 명령을 실행하십시오.

   ```txt
   /models
   ```

---

### LM Studio

LM Studio를 통해 로컬 모델을 사용하도록 구성할 수 있습니다.

```json title="opencode.json" "lmstudio" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "lmstudio": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "LM Studio (local)",
      "options": {
        "baseURL": "http://127.0.0.1:1234/v1"
      },
      "models": {
        "google/gemma-3n-e4b": {
          "name": "Gemma 3n-e4b (local)"
        }
      }
    }
  }
}
```

이 예제에서:

- `lmstudio`는 사용자 정의 공급자 ID입니다. 원하는 문자열로 지정할 수 있습니다.
- `npm`은 이 공급자에 사용할 패키지를 지정합니다. 여기서는 OpenAI 호환 API를 위해 `@ai-sdk/openai-compatible`을 사용합니다.
- `name`은 UI에 표시될 공급자 이름입니다.
- `options.baseURL`은 로컬 서버의 엔드포인트입니다.
- `models`는 모델 ID와 해당 구성을 매핑합니다. 모델 이름은 모델 선택 목록에 표시됩니다.

---

### Moonshot AI

Moonshot AI에서 Kimi K2를 사용하려면:

1. [Moonshot AI 콘솔](https://platform.moonshot.ai/console)로 이동하여 계정을 만들고 **Create API key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **Moonshot AI**를 검색하십시오.

   ```txt
   /connect
   ```

3. Moonshot API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 *Kimi K2*를 선택하십시오.

   ```txt
   /models
   ```

---

### MiniMax

1. [MiniMax API 콘솔](https://platform.minimax.io/login)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **MiniMax**를 검색하십시오.

   ```txt
   /connect
   ```

3. MiniMax API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _M2.1_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Nebius Token Factory

1. [Nebius Token Factory 콘솔](https://tokenfactory.nebius.com/)로 이동하여 계정을 만들고 **Add Key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **Nebius Token Factory**를 검색하십시오.

   ```txt
   /connect
   ```

3. Nebius Token Factory API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Kimi K2 Instruct_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Ollama

Ollama를 통해 로컬 모델을 사용하도록 구성할 수 있습니다.

:::tip
Ollama는 OpenCode에 대해 자동으로 구성될 수 있습니다. 자세한 내용은 [Ollama 통합 문서](https://docs.ollama.com/integrations/opencode)를 참조하십시오.
:::

```json title="opencode.json" "ollama" {5, 6, 8, 10-14}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "ollama": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "Ollama (local)",
      "options": {
        "baseURL": "http://localhost:11434/v1"
      },
      "models": {
        "llama2": {
          "name": "Llama 2"
        }
      }
    }
  }
}
```

이 예제에서:

- `ollama`는 사용자 정의 공급자 ID입니다. 원하는 문자열로 지정할 수 있습니다.
- `npm`은 이 공급자에 사용할 패키지를 지정합니다. 여기서는 OpenAI 호환 API를 위해 `@ai-sdk/openai-compatible`을 사용합니다.
- `name`은 UI에 표시될 공급자 이름입니다.
- `options.baseURL`은 로컬 서버의 엔드포인트입니다.
- `models`는 모델 ID와 해당 구성을 매핑합니다. 모델 이름은 모델 선택 목록에 표시됩니다.

:::tip
도구 호출이 작동하지 않는 경우, Ollama에서 `num_ctx`를 늘려보십시오. 16k - 32k 정도에서 시작하십시오.
:::

---

### Ollama Cloud

OpenCode로 Ollama Cloud를 사용하려면:

1. [https://ollama.com/](https://ollama.com/)으로 이동하여 로그인하거나 계정을 만듭니다.

2. **Settings** > **Keys**로 이동하여 **Add API Key**를 클릭해 새 API 키를 생성합니다.

3. OpenCode에서 사용할 API 키를 복사합니다.

4. `/connect` 명령을 실행하고 **Ollama Cloud**를 검색하십시오.

   ```txt
   /connect
   ```

5. Ollama Cloud API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

6. **중요**: OpenCode에서 클라우드 모델을 사용하기 전에, 로컬에서 모델 정보를 가져와야 합니다:

   ```bash
   ollama pull gpt-oss:20b-cloud
   ```

7. `/models` 명령을 실행하여 Ollama Cloud 모델을 선택하십시오.

   ```txt
   /models
   ```

---

### OpenAI

[ChatGPT Plus 또는 Pro](https://chatgpt.com/pricing)에 가입하는 것이 좋습니다.

1. 가입 후 `/connect` 명령을 실행하고 OpenAI를 선택하십시오.

   ```txt
   /connect
   ```

2. **ChatGPT Plus/Pro** 옵션을 선택하면 브라우저가 열리고 인증을 요청합니다.

   ```txt
   ┌ Select auth method
   │
   │ ChatGPT Plus/Pro
   │ Manually enter API Key
   └
   ```

3. 이제 `/models` 명령을 사용할 때 모든 OpenAI 모델을 사용할 수 있습니다.

   ```txt
   /models
   ```

##### API 키 사용

이미 API 키가 있다면 **Manually enter API Key**를 선택하고 터미널에 붙여넣을 수 있습니다.

---

### OpenCode Zen

OpenCode Zen은 OpenCode 팀에서 제공하는 테스트 및 검증된 모델 목록입니다. [더 알아보기](/docs/zen).

1. **<a href={console}>OpenCode Zen</a>**에 로그인하고 **Create API Key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **OpenCode Zen**을 검색하십시오.

   ```txt
   /connect
   ```

3. OpenCode API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Qwen 3 Coder 480B_)을 선택하십시오.

   ```txt
   /models
   ```

---

### OpenRouter

1. [OpenRouter 대시보드](https://openrouter.ai/settings/keys)로 이동하여 **Create API Key**를 클릭하고 키를 복사합니다.

2. `/connect` 명령을 실행하고 OpenRouter를 검색하십시오.

   ```txt
   /connect
   ```

3. API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 많은 OpenRouter 모델은 기본적으로 미리 로드되어 있으므로 `/models` 명령을 실행하여 원하는 것을 선택하십시오.

   ```txt
   /models
   ```

   OpenCode 구성을 통해 모델을 추가할 수도 있습니다.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

5. 또한 OpenCode 구성을 통해 사용자 정의할 수도 있습니다. 다음은 공급자 순서를 지정하는 예입니다.

   ```json title="opencode.json"
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "openrouter": {
         "models": {
           "moonshotai/kimi-k2": {
             "options": {
               "provider": {
                 "order": ["baseten"],
                 "allow_fallbacks": false
               }
             }
           }
         }
       }
     }
   }
   ```

---

### SAP AI Core

SAP AI Core는 OpenAI, Anthropic, Google, Amazon, Meta, Mistral 및 AI21의 40개 이상의 모델에 대한 액세스를 제공합니다.

1. [SAP BTP Cockpit](https://account.hana.ondemand.com/)으로 이동하여 SAP AI Core 서비스 인스턴스로 이동하고 서비스 키를 만듭니다.

   :::tip
   서비스 키는 `clientid`, `clientsecret`, `url` 및 `serviceurls.AI_API_URL`을 포함하는 JSON 객체입니다. **Services** > **Instances and Subscriptions** 아래에서 AI Core 인스턴스를 찾을 수 있습니다.
   :::

2. `/connect` 명령을 실행하고 **SAP AI Core**를 검색하십시오.

   ```txt
   /connect
   ```

3. 서비스 키 JSON을 입력하십시오.

   ```txt
   ┌ Service key
   │
   │
   └ enter
   ```

   또는 `AICORE_SERVICE_KEY` 환경 변수를 설정합니다:

   ```bash
   AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}' opencode
   ```

   또는 bash 프로필에 추가합니다:

   ```bash title="~/.bash_profile"
   export AICORE_SERVICE_KEY='{"clientid":"...","clientsecret":"...","url":"...","serviceurls":{"AI_API_URL":"..."}}'
   ```

4. 선택적으로 배포 ID 및 리소스 그룹을 설정합니다:

   ```bash
   AICORE_DEPLOYMENT_ID=your-deployment-id AICORE_RESOURCE_GROUP=your-resource-group opencode
   ```

   :::note
   이 설정은 선택 사항이며 SAP AI Core 설정에 따라 구성해야 합니다.
   :::

5. `/models` 명령을 실행하여 40개 이상의 사용 가능한 모델 중에서 선택하십시오.

   ```txt
   /models
   ```

---

### STACKIT

STACKIT AI Model Serving은 Llama, Mistral, Qwen과 같은 LLM에 초점을 맞추고 유럽 인프라에서 데이터 주권을 최대한 보장하는 완전 관리형 AI 모델 호스팅 환경을 제공합니다.

1. [STACKIT Portal](https://portal.stackit.cloud)로 이동하여 **AI Model Serving**으로 이동한 다음 프로젝트의 인증 토큰을 만듭니다.

   :::tip
   인증 토큰을 만들기 전에 STACKIT 고객 계정, 사용자 계정 및 프로젝트가 필요합니다.
   :::

2. `/connect` 명령을 실행하고 **STACKIT**을 검색하십시오.

   ```txt
   /connect
   ```

3. STACKIT AI Model Serving 인증 토큰을 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 _Qwen3-VL 235B_ 또는 *Llama 3.3 70B*와 같은 사용 가능한 모델을 선택하십시오.

   ```txt
   /models
   ```

---

### OVHcloud AI Endpoints

1. [OVHcloud 패널](https://ovh.com/manager)로 이동합니다. `Public Cloud` 섹션으로 이동하여 `AI & Machine Learning` > `AI Endpoints`로 간 뒤 `API Keys` 탭에서 **Create a new API key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **OVHcloud AI Endpoints**를 검색하십시오.

   ```txt
   /connect
   ```

3. OVHcloud AI Endpoints API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _gpt-oss-120b_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Scaleway

OpenCode로 [Scaleway Generative APIs](https://www.scaleway.com/en/docs/generative-apis/)를 사용하려면:

1. [Scaleway Console IAM 설정](https://console.scaleway.com/iam/api-keys)에서 새 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Scaleway**를 검색하십시오.

   ```txt
   /connect
   ```

3. Scaleway API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _devstral-2-123b-instruct-2512_ 또는 _gpt-oss-120b_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Together AI

1. [Together AI 콘솔](https://api.together.ai)로 이동하여 계정을 만들고 **Add Key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **Together AI**를 검색하십시오.

   ```txt
   /connect
   ```

3. Together AI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Kimi K2 Instruct_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Venice AI

1. [Venice AI 콘솔](https://venice.ai)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Venice AI**를 검색하십시오.

   ```txt
   /connect
   ```

3. Venice AI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Llama 3.3 70B_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Vercel AI Gateway

Vercel AI Gateway는 OpenAI, Anthropic, Google, xAI 등의 모델에 액세스할 수 있으며, 통합된 엔드포인트를 통해 더 많은 기능을 제공합니다. 모델은 마크업 없이 정가로 제공됩니다.

1. [Vercel 대시보드](https://vercel.com/)로 이동하여 **AI Gateway** 탭으로 간 뒤, **API keys**를 클릭하여 새 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **Vercel AI Gateway**를 검색하십시오.

   ```txt
   /connect
   ```

3. Vercel AI Gateway API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델을 선택하십시오.

   ```txt
   /models
   ```

OpenCode 구성을 통해 모델을 사용자 정의할 수도 있습니다. 다음은 공급자 라우팅 순서를 지정하는 예입니다.

```json title="opencode.json"
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "vercel": {
      "models": {
        "anthropic/claude-sonnet-4": {
          "options": {
            "order": ["anthropic", "vertex"]
          }
        }
      }
    }
  }
}
```

몇 가지 유용한 라우팅 옵션:

| 옵션                | 설명                              |
| ------------------- | --------------------------------- |
| `order`             | 시도할 공급자 순서                |
| `only`              | 특정 공급자로 제한                |
| `zeroDataRetention` | 데이터 보유 정책이 없는 곳만 사용 |

---

### xAI

1. [xAI 콘솔](https://console.x.ai/)로 이동하여 계정을 만들고 API 키를 생성합니다.

2. `/connect` 명령을 실행하고 **xAI**를 검색하십시오.

   ```txt
   /connect
   ```

3. xAI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _Grok Beta_)을 선택하십시오.

   ```txt
   /models
   ```

---

### Z.AI

1. [Z.AI API 콘솔](https://z.ai/manage-apikey/apikey-list)로 이동하여 계정을 만들고 **Create a new API key**를 클릭합니다.

2. `/connect` 명령을 실행하고 **Z.AI**를 검색하십시오.

   ```txt
   /connect
   ```

   **GLM Coding Plan**에 가입했다면 **Z.AI Coding Plan**을 선택하십시오.

3. Z.AI API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. `/models` 명령을 실행하여 모델(예: _GLM-4.7_)을 선택하십시오.

   ```txt
   /models
   ```

---

### ZenMux

1. [ZenMux 대시보드](https://zenmux.ai/settings/keys)로 이동하여 **Create API Key**를 클릭하고 키를 복사합니다.

2. `/connect` 명령을 실행하고 ZenMux를 검색하십시오.

   ```txt
   /connect
   ```

3. API 키를 입력하십시오.

   ```txt
   ┌ API key
   │
   │
   └ enter
   ```

4. 많은 ZenMux 모델은 기본적으로 미리 로드되어 있으므로 `/models` 명령을 실행하여 원하는 것을 선택하십시오.

   ```txt
   /models
   ```

   OpenCode 구성을 통해 모델을 추가할 수도 있습니다.

   ```json title="opencode.json" {6}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "zenmux": {
         "models": {
           "somecoolnewmodel": {}
         }
       }
     }
   }
   ```

---

## 사용자 정의 공급자

`/connect` 명령에 나열되지 않은 **OpenAI 호환** 공급자를 추가하려면:

:::tip
OpenCode에서 모든 OpenAI 호환 공급자를 사용할 수 있습니다. 대부분의 최신 AI 공급자는 OpenAI 호환 API를 제공합니다.
:::

1. `/connect` 명령을 실행하고 **Other**로 스크롤하십시오.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◆  Select provider
   │  ...
   │  ● Other
   └
   ```

2. 공급자를 위한 고유한 ID를 입력하십시오.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ◇  Enter provider id
   │  myprovider
   └
   ```

   :::note
   기억하기 쉬운 ID를 선택하십시오. 구성 파일에서 이 ID를 사용하게 됩니다.
   :::

3. 공급자의 API 키를 입력하십시오.

   ```bash
   $ /connect

   ┌  Add credential
   │
   ▲  This only stores a credential for myprovider - you will need to configure it in opencode.json, check the docs for examples.
   │
   ◇  Enter your API key
   │  sk-...
   └
   ```

4. 프로젝트 디렉토리에 `opencode.json` 파일을 만들거나 업데이트하십시오.

   ```json title="opencode.json" ""myprovider"" {5-15}
   {
     "$schema": "https://opencode.ai/config.json",
     "provider": {
       "myprovider": {
         "npm": "@ai-sdk/openai-compatible",
         "name": "My AI ProviderDisplay Name",
         "options": {
           "baseURL": "https://api.myprovider.com/v1"
         },
         "models": {
           "my-model-name": {
             "name": "My Model Display Name"
           }
         }
       }
     }
   }
   ```

   구성 옵션은 다음과 같습니다:
   - **npm**: 사용할 AI SDK 패키지. OpenAI 호환 공급자의 경우 `@ai-sdk/openai-compatible`
   - **name**: UI에 표시될 이름
   - **models**: 사용 가능한 모델
   - **options.baseURL**: API 엔드포인트 URL
   - **options.apiKey**: 인증을 사용하지 않는 경우 선택적으로 API 키 설정
   - **options.headers**: 선택적으로 사용자 정의 헤더 설정

   고급 옵션에 대한 자세한 내용은 아래 예제를 참조하십시오.

5. `/models` 명령을 실행하면 사용자 정의 공급자와 모델이 선택 목록에 나타납니다.

---

##### 예제

다음은 `apiKey`, `headers` 및 모델 `limit` 옵션을 설정하는 예입니다.

```json title="opencode.json" {9,11,17-20}
{
  "$schema": "https://opencode.ai/config.json",
  "provider": {
    "myprovider": {
      "npm": "@ai-sdk/openai-compatible",
      "name": "My AI ProviderDisplay Name",
      "options": {
        "baseURL": "https://api.myprovider.com/v1",
        "apiKey": "{env:ANTHROPIC_API_KEY}",
        "headers": {
          "Authorization": "Bearer custom-token"
        }
      },
      "models": {
        "my-model-name": {
          "name": "My Model Display Name",
          "limit": {
            "context": 200000,
            "output": 65536
          }
        }
      }
    }
  }
}
```

구성 세부 사항:

- **apiKey**: `env` 변수 구문을 사용하여 설정, [더 알아보기](/docs/config#env-vars).
- **headers**: 각 요청과 함께 전송되는 사용자 정의 헤더.
- **limit.context**: 모델이 허용하는 최대 입력 토큰.
- **limit.output**: 모델이 생성할 수 있는 최대 출력 토큰.

`limit` 필드를 사용하면 OpenCode가 남은 컨텍스트 양을 파악할 수 있습니다. 표준 공급자는 models.dev에서 자동으로 이를 가져옵니다.

---

## 문제 해결

공급자 구성에 문제가 있는 경우 다음을 확인하십시오.

1. **인증 설정 확인**: `opencode auth list`를 실행하여 공급자의 자격 증명이 구성에 추가되었는지 확인하십시오.

   Amazon Bedrock과 같이 인증을 위해 환경 변수에 의존하는 공급자에는 적용되지 않습니다.

2. 사용자 정의 공급자의 경우, OpenCode 구성을 확인하고 다음을 수행하십시오:
   - `/connect` 명령에 사용된 공급자 ID가 OpenCode 구성의 ID와 일치하는지 확인하십시오.
   - 공급자에 올바른 npm 패키지가 사용되었는지 확인하십시오. 예를 들어 Cerebras에는 `@ai-sdk/cerebras`를 사용하고, 다른 모든 OpenAI 호환 공급자에는 `@ai-sdk/openai-compatible`을 사용하십시오.
   - `options.baseURL` 필드에 올바른 API 엔드포인트가 사용되었는지 확인하십시오.
